import { createWriteStream } from "fs";
import { dedent } from "ts-dedent";
import { exec } from "child_process";
import * as fs from "fs/promises";
import * as z from "zod";
import decompress from "decompress";
import getPort from "get-port";
import http from "node:http";
import https from "node:https";
import Module from "node:module";
import path from "path";
import { buildDatabaseURL, parsePgProxyPort, appRoot } from '../util/index.js';
import { buildMigrations, getMigrationNames } from './builder.js';
import { findAndReplaceInFile } from '../util/index.js';
import { getConfig } from '../config.js';
import { start } from '../docker-commands/command-start.js';
import { stop } from '../docker-commands/command-stop.js';
import { withConfig } from '../configure/command-with-config.js';
import { pgBuilder, sqliteBuilder } from '../../migrators/query-builder/index.js';
const require2 = Module.createRequire(import.meta.url);
const prismaPath = require2.resolve("prisma");
const generatorPath = path.join(
  path.dirname(require2.resolve("@electric-sql/prisma-generator")),
  "bin.js"
);
const sqliteMigrationsFileName = "migrations.ts";
const pgMigrationsFileName = "pg-migrations.ts";
const defaultPollingInterval = 1e3;
async function generate(options) {
  const opts = {
    exitOnError: true,
    ...options
  };
  let config = opts.config;
  if (opts.watch && opts.withMigrations) {
    console.error(
      "Cannot use --watch and --with-migrations at the same time. Please choose one."
    );
    process.exit(1);
  }
  console.log("Generating Electric client...");
  try {
    if (opts.withMigrations) {
      console.log("Starting ElectricSQL and PostgreSQL containers...");
      delete process.env.ELECTRIC_SERVICE;
      delete process.env.ELECTRIC_PROXY;
      config = getConfig({
        ...config,
        SERVICE: void 0,
        PROXY: void 0,
        ...await withMigrationsConfig(config.CONTAINER_NAME)
      });
      opts.config = config;
      await start({
        config,
        withPostgres: true,
        detach: true,
        exitOnDetached: false
      });
      console.log("Running migrations...");
      const ret = withConfig(opts.withMigrations, opts.config);
      if (ret.status !== 0) {
        console.log(
          "Failed to run migrations, --with-migrations command exited with error"
        );
        process.exit(1);
      }
    }
    console.log("Service URL: " + opts.config.SERVICE);
    console.log("Proxy URL: " + stripPasswordFromUrl(opts.config.PROXY));
    if (opts.watch) {
      watchMigrations(opts);
    } else {
      await _generate(opts);
    }
  } finally {
    if (opts.withMigrations) {
      console.log("Stopping ElectricSQL and PostgreSQL containers...");
      await stop({
        remove: true,
        config
      });
      console.log("Done");
    }
  }
}
async function watchMigrations(opts) {
  const config = opts.config;
  const pollingInterval = opts.pollingInterval;
  const pollMigrations = async () => {
    const tmpFolder = await fs.mkdtemp(".electric_migrations_tmp_");
    try {
      const latestMigration = await getLatestMigration(opts);
      let migrationEndpoint = config.SERVICE + "/api/migrations?dialect=sqlite";
      if (latestMigration !== void 0) {
        migrationEndpoint = migrationEndpoint + `&version=${latestMigration}`;
      }
      const migrationsPath = path.join(tmpFolder, "migrations");
      await fs.mkdir(migrationsPath);
      const migrationsFolder = path.resolve(migrationsPath);
      const gotNewMigrations = await fetchMigrations(
        migrationEndpoint,
        migrationsFolder,
        tmpFolder
      );
      if (gotNewMigrations) {
        const newMigrations = await getMigrationNames(migrationsFolder);
        console.info("Discovered new migrations: " + newMigrations.join(", "));
        await _generate(opts);
      }
    } catch (e) {
      console.error(JSON.stringify(e));
    } finally {
      await fs.rm(tmpFolder, { recursive: true });
      setTimeout(pollMigrations, pollingInterval);
    }
  };
  pollMigrations();
}
async function getLatestMigration(opts) {
  const migrationsFile = migrationsFilePath(opts, "SQLite");
  let migrations = void 0;
  let migrationsFileContent = "";
  try {
    migrationsFileContent = await fs.readFile(migrationsFile, "utf8");
  } catch (e) {
    return void 0;
  }
  const migrationsSchema = z.object({
    statements: z.string().array(),
    version: z.string()
  }).array();
  try {
    const prefix = "export default ";
    const migrationsStr = migrationsFileContent.substring(prefix.length);
    migrations = JSON.parse(migrationsStr);
    const parsedMigrations = migrationsSchema.parse(migrations);
    if (parsedMigrations.length === 0)
      return void 0;
    else {
      const lastMigration = parsedMigrations[parsedMigrations.length - 1];
      return lastMigration.version;
    }
  } catch (e) {
    throw new Error(
      "Could not read migrations because they have an unexpected format."
    );
  }
}
async function bundleMigrationsFor(dialect, opts, tmpFolder) {
  const config = opts.config;
  const folder = dialect === "SQLite" ? "migrations" : "pg-migrations";
  const migrationsPath = path.join(tmpFolder, folder);
  await fs.mkdir(migrationsPath);
  const dialectArg = dialect === "SQLite" ? "sqlite" : "postgresql";
  const migrationEndpoint = config.SERVICE + `/api/migrations?dialect=${dialectArg}`;
  const migrationsFolder = path.resolve(migrationsPath);
  const migrationsFile = migrationsFilePath(opts, dialect);
  await fetchMigrations(migrationEndpoint, migrationsFolder, tmpFolder);
  const builder = dialect === "SQLite" ? sqliteBuilder : pgBuilder;
  return async () => {
    await buildMigrations(migrationsFolder, migrationsFile, builder);
  };
}
async function _generate(opts) {
  const config = opts.config;
  const tmpFolder = await fs.mkdtemp(".electric_migrations_tmp_");
  let generationFailed = false;
  try {
    const buildSqliteMigrations = await bundleMigrationsFor(
      "SQLite",
      opts,
      tmpFolder
    );
    const buildPgMigrations = await bundleMigrationsFor(
      "Postgres",
      opts,
      tmpFolder
    );
    const prismaSchema = await createIntrospectionSchema(tmpFolder, opts);
    await introspectDB(prismaSchema);
    await generateClient(prismaSchema, config.CLIENT_PATH);
    const relativePath = path.relative(appRoot, config.CLIENT_PATH);
    console.log(`Successfully generated Electric client at: ./${relativePath}`);
    console.log("Building migrations...");
    await buildSqliteMigrations();
    await buildPgMigrations();
    console.log("Successfully built migrations");
    if (["nodenext", "node16"].includes(
      config.MODULE_RESOLUTION.toLocaleLowerCase()
    )) {
      await rewriteImportsForNodeNext(config.CLIENT_PATH);
    }
  } catch (e) {
    generationFailed = true;
    console.error("generate command failed: " + e);
    throw e;
  } finally {
    if (!opts.debug)
      await fs.rm(tmpFolder, { recursive: true });
    if (generationFailed && opts.exitOnError)
      process.exit(1);
  }
}
async function generateClient(prismaSchema, clientPath) {
  await addValidators(prismaSchema);
  await capitaliseTableNames(prismaSchema);
  const introspectedSchema = await fs.readFile(prismaSchema, "utf8");
  await createElectricClientSchema(introspectedSchema, prismaSchema, clientPath);
  await generateElectricClient(prismaSchema);
  await createPrismaClientSchema(introspectedSchema, prismaSchema, clientPath);
  await generatePrismaClient(prismaSchema);
  await augmentGeneratedClient(clientPath);
}
async function augmentGeneratedClient(clientPath) {
  await extendJsonType(clientPath);
  await replaceByteArrayType(clientPath);
  await keepOnlyPrismaTypings(clientPath);
}
function escapePathForString(inputPath) {
  return process.platform === "win32" ? inputPath.replace(/\\/g, "\\\\") : inputPath;
}
function buildProxyUrlForIntrospection(config) {
  return buildDatabaseURL({
    user: "prisma",
    // We use the "prisma" user to put the proxy into introspection mode
    password: config.PG_PROXY_PASSWORD,
    host: config.PG_PROXY_HOST,
    port: parsePgProxyPort(config.PG_PROXY_PORT).port,
    dbName: config.DATABASE_NAME
  });
}
async function createIntrospectionSchema(folder, opts) {
  const config = opts.config;
  const prismaDir = path.join(folder, "prisma");
  const prismaSchemaFile = path.join(prismaDir, "schema.prisma");
  await fs.mkdir(prismaDir);
  const proxyUrl = buildProxyUrlForIntrospection(config);
  const schema = dedent`
    datasource db {
      provider = "postgresql"
      url      = "${proxyUrl}"
    }`;
  await fs.writeFile(prismaSchemaFile, schema);
  return prismaSchemaFile;
}
async function createElectricClientSchema(introspectedSchema, prismaSchemaFile, clientPath) {
  const output = path.resolve(clientPath);
  const schema = dedent`
    generator electric {
      provider      = "node ${escapePathForString(generatorPath)}"
      output        = "${escapePathForString(output)}"
      relationModel = "false"
    }
    
    ${introspectedSchema}`;
  await fs.writeFile(prismaSchemaFile, schema);
  return prismaSchemaFile;
}
async function createPrismaClientSchema(introspectedSchema, prismaSchemaFile, clientPath) {
  const output = path.resolve(clientPath);
  const schema = dedent`
    generator client {
      provider = "prisma-client-js"
      output   = "${escapePathForString(output)}"
    }
    
    ${introspectedSchema}`;
  await fs.writeFile(prismaSchemaFile, schema);
  return prismaSchemaFile;
}
async function getFileLines(prismaSchema) {
  const contents = await fs.readFile(prismaSchema, "utf8");
  return contents.split(/\r?\n/);
}
async function capitaliseTableNames(prismaSchema) {
  const lines = await getFileLines(prismaSchema);
  const casedLines = doCapitaliseTableNames(lines);
  await fs.writeFile(prismaSchema, casedLines.join("\n"));
}
function doCapitaliseTableNames(lines) {
  const replacements = /* @__PURE__ */ new Map();
  const modelNameToDbName = /* @__PURE__ */ new Map();
  const modelRegex = /^\s*model\s+([A-Za-z][A-Za-z0-9_]*)\s*{/;
  const getModelName = (ln) => ln.match(modelRegex)?.[1];
  lines.forEach((ln, idx) => {
    const tableName = getModelName(ln);
    if (tableName) {
      const modelName2 = capitaliseFirstLetter(tableName);
      const newLn = ln.replace(modelRegex, (_, _tableName) => {
        return `model ${modelName2} {`;
      });
      lines[idx] = newLn;
      replacements.set(tableName, modelName2);
      modelNameToDbName.set(modelName2, tableName);
    }
  });
  let modelName;
  let modelHasMapAttribute = false;
  const insideModel = () => modelName !== void 0;
  lines = lines.flatMap((ln) => {
    modelName = getModelName(ln) ?? modelName;
    if (insideModel() && ln.trim().startsWith("}")) {
      const tableName = modelNameToDbName.get(modelName);
      modelName = void 0;
      if (!modelHasMapAttribute) {
        return [`  @@map("${tableName}")`, ln];
      }
      modelHasMapAttribute = false;
      return ln;
    }
    const nameMappingRegex = /^\s*@@map\("(.*)"\)\s*$/;
    const mapAttribute = ln.match(nameMappingRegex);
    if (insideModel() && mapAttribute !== null) {
      modelHasMapAttribute = true;
      const originalTableName = mapAttribute[1];
      modelNameToDbName.set(modelName, originalTableName);
    }
    if (insideModel()) {
      const reg = /^(\s*\w+\s+)(\w+)/;
      return ln.replace(reg, (_match, columnName, typeName) => {
        const newTypeName = replacements.get(typeName) ?? typeName;
        return columnName + newTypeName;
      });
    }
    return ln;
  });
  return lines;
}
async function introspectDB(prismaSchema) {
  await executeShellCommand(
    `node ${prismaPath} db pull --schema="${prismaSchema}"`,
    "Introspection script exited with error code: "
  );
}
async function addValidators(prismaSchema) {
  const lines = await getFileLines(removeComments(prismaSchema));
  const newLines = lines.map(addValidator);
  await fs.writeFile(prismaSchema, newLines.join("\n"));
}
function addValidator(ln) {
  const field = parseFields(ln)[0];
  if (field) {
    const intValidator = "@zod.number.int().gte(-2147483648).lte(2147483647)";
    const floatValidator = "@zod.custom.use(z.number().or(z.nan()))";
    const attributeValidatorMapping = /* @__PURE__ */ new Map([
      ["@db.Uuid", "@zod.string.uuid()"],
      ["@db.SmallInt", "@zod.number.int().gte(-32768).lte(32767)"],
      ["@db.Int", intValidator],
      ["@db.DoublePrecision", floatValidator],
      ["@db.Real", floatValidator]
    ]);
    const attribute = field.attributes.map((a) => a.type).find((a) => attributeValidatorMapping.has(a));
    if (attribute) {
      return ln + " /// " + attributeValidatorMapping.get(attribute);
    } else {
      const typeValidatorMapping = /* @__PURE__ */ new Map([
        ["Int", intValidator],
        ["Int?", intValidator],
        ["Int[]", intValidator],
        ["Float", floatValidator],
        ["Float?", floatValidator],
        ["Float[]", floatValidator]
      ]);
      const typeValidator = typeValidatorMapping.get(field.type);
      if (typeValidator) {
        return ln + " /// " + typeValidator;
      } else {
        return ln;
      }
    }
  } else {
    return ln;
  }
}
async function generateElectricClient(prismaSchema) {
  await executeShellCommand(
    `node ${prismaPath} generate --schema="${prismaSchema}"`,
    "Generator script exited with error code: "
  );
}
async function generatePrismaClient(prismaSchema) {
  await executeShellCommand(
    `node ${prismaPath} generate --schema="${prismaSchema}"`,
    "Generator script exited with error code: "
  );
}
async function executeShellCommand(command, errMsg) {
  return new Promise((resolve, reject) => {
    const proc = exec(command, { cwd: appRoot }, (error, _stdout, _stderr) => {
      if (error) {
        console.error(error);
      }
    });
    proc.on("close", (code) => {
      if (code === 0) {
        resolve();
      } else {
        reject(errMsg + code);
      }
    });
  });
}
async function fetchMigrations(endpoint, writeTo, tmpFolder) {
  const options = new URL(endpoint);
  const zipFile = path.join(tmpFolder, "migrations.zip");
  const requestModule = options.protocol === "http:" ? http : options.protocol === "https:" ? https : void 0;
  if (requestModule === void 0)
    throw new TypeError(
      `Protocol "${options.protocol}" not supported. Expected "http:" or "https:"`
    );
  const gotNewMigrations = await new Promise((resolve, reject) => {
    const req = requestModule.get(options, (response) => {
      if (response.statusCode === 204) {
        resolve(false);
      } else if (response.statusCode === 200) {
        const migrationsZipFile = createWriteStream(zipFile);
        response.pipe(migrationsZipFile);
        migrationsZipFile.on("finish", () => resolve(true));
      } else {
        reject(
          `Failed to fetch migrations from Electric. Got ${response.statusCode} status code.`
        );
      }
    });
    req.on("error", reject);
  });
  if (gotNewMigrations) {
    await decompress(zipFile, writeTo);
  }
  return gotNewMigrations;
}
function migrationsFilePath(opts, sqlDialect) {
  const outFolder = path.resolve(opts.config.CLIENT_PATH);
  const migrationsFileName = sqlDialect === "SQLite" ? sqliteMigrationsFileName : pgMigrationsFileName;
  return path.join(outFolder, migrationsFileName);
}
function capitaliseFirstLetter(word) {
  return word.charAt(0).toUpperCase() + word.substring(1);
}
function removeComments(str) {
  const commentRegex = /(?<=[^/])\/\/(?=[^/]).*$/g;
  return str.replaceAll(commentRegex, "");
}
function parseFields(body) {
  const fieldRegex = /^\s*(?<field>\w+)\s+(?<type>[\w]+(\?|(\[]))?)\s*(?<attributes>((@[\w.]+\s*)|(@[\w.]+\(.*\)+\s*))+)?\s*$/gm;
  const fieldMatches = [...body.matchAll(fieldRegex)];
  const fs2 = fieldMatches.map(
    (match) => match.groups
  );
  return fs2.map((f) => ({
    ...f,
    attributes: parseAttributes(f.attributes ?? "")
  }));
}
function parseAttributes(attributes) {
  const attributeRegex = /(?<type>@[\w.]+)(?<args>\([^@\n\r]+\))?/g;
  const matches = [...attributes.matchAll(attributeRegex)];
  return matches.map((m) => {
    const { type, args } = m.groups;
    const noParens = args?.substring(1, args.length - 1);
    const parsedArgs = noParens?.split(",")?.map((arg) => arg.trim()) ?? [];
    return {
      type,
      args: parsedArgs
    };
  });
}
function extendJsonType(prismaDir) {
  const prismaTypings = path.join(prismaDir, "index.d.ts");
  const inputJsonValueRegex = /^\s*export\s*type\s*InputJsonValue\s*(=)\s*/gm;
  const replacement = "export type InputJsonValue = null | ";
  return findAndReplaceInFile(inputJsonValueRegex, replacement, prismaTypings);
}
function replaceByteArrayType(prismaDir) {
  const prismaTypings = path.join(prismaDir, "index.d.ts");
  return fs.appendFile(
    prismaTypings,
    // omit 'set' property as it conflicts with the DAL setter prop name
    "\n\ntype Buffer = Omit<Uint8Array, 'set'>\n"
  );
}
async function keepOnlyPrismaTypings(prismaDir) {
  const contents = await fs.readdir(prismaDir);
  const proms = contents.map(async (fileOrDir) => {
    const filePath = path.join(prismaDir, fileOrDir);
    if (fileOrDir === "index.d.ts") {
      return fs.rename(filePath, path.join(prismaDir, "prismaClient.d.ts"));
    } else if (fileOrDir !== "index.ts") {
      return fs.rm(filePath, { recursive: true });
    }
  });
  await Promise.all(proms);
}
async function rewriteImportsForNodeNext(clientDir) {
  const file = path.join(clientDir, "index.ts");
  const content = await fs.readFile(file, "utf8");
  const newContent = content.replace("from './migrations';", "from './migrations.js';").replace("from './prismaClient';", "from './prismaClient.js';");
  await fs.writeFile(file, newContent);
}
async function withMigrationsConfig(containerName) {
  return {
    HTTP_PORT: await getPort(),
    PG_PROXY_PORT: (await getPort()).toString(),
    DATABASE_PORT: await getPort(),
    SERVICE_HOST: "localhost",
    PG_PROXY_HOST: "localhost",
    DATABASE_REQUIRE_SSL: false,
    // Random container name to avoid collisions
    CONTAINER_NAME: `${containerName}-migrations-${Math.random().toString(36).slice(6)}`
  };
}
function stripPasswordFromUrl(url) {
  const parsed = new URL(url);
  if (parsed.password) {
    parsed.password = "********";
  }
  return parsed.toString();
}
export {
  defaultPollingInterval,
  doCapitaliseTableNames,
  generate,
  generateClient
};
//# sourceMappingURL=migrate.js.map